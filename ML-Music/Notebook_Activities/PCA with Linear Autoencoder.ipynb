{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Name:\n",
    "#### Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with Linear Autoencoder\n",
    "\n",
    "Instructions: \n",
    "\n",
    "* This notebook is an interactive assignment; please read and follow the instructions in each cell. \n",
    "\n",
    "* Cells that require your input (in the form of code or written response) will have 'Question #' above.\n",
    "\n",
    "* After completing the assignment, please submit this notebook as a PDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run the following commands to set up your environment:\n",
    "\n",
    "`pip install tensorflow`\n",
    "\n",
    "`pip install keras`\n",
    "\n",
    "`pip install --upgrade protobuf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rossg\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import wavfile\n",
    "from numpy.linalg import svd\n",
    "from scipy.stats.mstats import gmean\n",
    "from matplotlib import rcParams\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Lambda\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with a Linear Autoencoder\n",
    "\n",
    "In this problem, we will practice using basic neural network procedures by running an autoencoder network.\n",
    "This network is implemented in TensorFlow (essentially Keras with an expanded toolset); the functions we call are nearly identical to those you will see in Keras. \n",
    "\n",
    "Let's create a sin+noise signal to use as input to our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 203\n",
    "fs = 10000\n",
    "T = 3\n",
    "Ns = fs*T\n",
    "\n",
    "def buffer(input_data, sample_rate, window_size, hop_size):\n",
    "    output = np.array([input_data[i:i+window_size] for i in range(0, len(input_data)-window_size, hop_size)])\n",
    "    return output.T\n",
    "\n",
    "s = np.sin(2*np.pi*f0*np.arange(Ns)/fs)\n",
    "\n",
    "n = np.random.randn(Ns)\n",
    "\n",
    "x = s + 0.3*n \n",
    "\n",
    "plt.plot(x[:1000])\n",
    "wavfile.write('out2.wav', fs, x)\n",
    "xmat = buffer(x,fs,400,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an AE with 2 hidden layers. \n",
    "\n",
    "This neural network is implemented in TensorFlow. \n",
    "\n",
    "Please review the code cells below, and answer the questions that follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = np.shape(xmat)[0]\n",
    "n_hidden = 2 \n",
    "\n",
    "learning_rate = 0.01 \n",
    "\n",
    "X = tf.compat.v1.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "W = tf.Variable(tf.truncated_normal(stddev=.1, shape =[n_inputs,n_hidden]))\n",
    "\n",
    "hidden = tf.matmul(X,W)\n",
    "outputs = tf.matmul(hidden,tf.transpose(W))\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10000\n",
    "codings = hidden\n",
    "X_train = xmat.T\n",
    "X_test = X_train\n",
    "\n",
    "col = ['b','r','g','c','m','y','k']\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "    \n",
    "for iteration in range(n_iterations):\n",
    "    training_op.run(feed_dict={X: X_train})\n",
    "\n",
    "    if iteration %1000 == 0:\n",
    "        W_val = W.eval()\n",
    "        plt.clf()\n",
    "        for k in range(n_hidden):\n",
    "            plt.subplot(n_hidden,1,k+1)\n",
    "            plt.plot(W_val[:,k],col[k % len(col)])\n",
    "        plt.show(False)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "codings_val = codings.eval(feed_dict={X: X_test})\n",
    "\n",
    "print(\"Done with training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an autoencoder? Please explain briefly. What would happen (ideally) if you pass a portion of signal x through the trained network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2 (10 points)\n",
    "\n",
    "Based on the observed shape of n_inputs and the definition of X_train, what exactly is being passed to the input layer of the network for a single forward pass? Be specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3 (10 points)\n",
    "\n",
    "What variable(s) are used to represent the network weights? How are these weights initialized prior to training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question  4 (10 points)\n",
    "\n",
    "What is being minimized in the reconstruction loss? Why is this helpful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5 (10 points)\n",
    "\n",
    "What is an optimizer? What are 3 common optimizers? Which optimizer is used in this AE training? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine now the \"codings\", i.e. the hidden unit values and their distribution. The more signigicant codings should have smaller variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(codings_val[:,0],codings_val[:,1],'.')\n",
    "print (\"mean: \", np.mean(codings_val,0))\n",
    "print (\"variance\", np.std(codings_val,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 6 (10 points)\n",
    "\n",
    "In what way does the autoencoder network function similarly to PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA versus AE (30 points)\n",
    "\n",
    "Using sklearn.decomposition.PCA (or another PCA implementation in Python), illustrate the ideas you discuss in Question 1 & Question 6. You should have two plots: one showing the output of the AE network on a single sample, and another showing the output of your PCA on the same sample. How many principal components do you use in your reconstruction to achieve similar performance to the AE network? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your code & plots here. \n",
    "Please make sure the number of principal components used is clear from your code.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
